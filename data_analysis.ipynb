{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The First part of this script is about the setup of the data collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from the file\n",
    "data = pd.read_csv('D:\\OneDrive - Texas Tech University\\PUBLICATIONS\\In Progress\\complex_supply_chain\\data_output\\data.csv')\n",
    "#data = pd.read_csv(r\"C:\\Users\\gar14685\\OneDrive - Texas Tech University\\PUBLICATIONS\\In Progress\\complex_supply_chain\\data_output\\data.csv\")\n",
    "#data = pd.read_csv('/Users/memesmacbookair/Library/CloudStorage/OneDrive-TexasTechUniversity/PUBLICATIONS/In Progress/complex_supply_chain/data_output/data.csv')\n",
    "\n",
    "# Deleting observations without answers to the Choice Experiments\n",
    "data[['Q30','Q32','Q34','Q36','Q38','Q40','Q42','Q44','Q46','Q48','Q50','Q52']] = data[['Q30','Q32','Q34','Q36','Q38','Q40','Q42','Q44','Q46','Q48','Q50','Q52']].fillna(value=0)\n",
    "\n",
    "data = data.drop(data[(data['Q30'] == 0) | (data['Q32'] == 0) | (data['Q34'] == 0) | (data['Q36'] == 0) | (data['Q38'] == 0) | (data['Q40'] == 0) | (data['Q42'] == 0) | (data['Q44'] == 0) | (data['Q46'] == 0) | (data['Q48'] == 0) | (data['Q50'] == 0) | (data['Q52'] == 0)].index)\n",
    "\n",
    "data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a range of integers for the ID column\n",
    "data = data.assign(ID=range(1, len(data)+1))\n",
    "\n",
    "#print(data['ID'].tail())\n",
    "data.loc[:,['ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat each row 36 times\n",
    "data = pd.concat([data]*36, ignore_index=True).sort_values(by='ID')\n",
    "\n",
    "# Create a column with the values 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 repeated three times\n",
    "data[\"Set\"] = [(i//3)%12 + 1 for i in range(len(data))]\n",
    "\n",
    "#data.loc[:,['ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort data by Block and Alt\n",
    "data2 = data.sort_values(by=['Block', 'ID', 'Set',])\n",
    "\n",
    "#data2.iloc[:40,-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column \"Alt\" with values 1, 2, 3, repeating for each row\n",
    "data2[\"Alt\"] = [i%3 + 1 for i in range(len(data2))]\n",
    "\n",
    "#data2.iloc[:40,-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column \"Q\" with values i1, i2, i3, for i = abcdefghijkl, repeating for each row\n",
    "n_rows = data2.shape[0]\n",
    "\n",
    "# Define the values for column \"Q\"\n",
    "values = [f'{letter}{i}' for letter in 'abcdefghijkl' for i in range(1, 4)] * (n_rows // 12 + 1)\n",
    "\n",
    "# Create the column \"Q\" with the values\n",
    "data2['Q'] = values[:n_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning values of 1 to the choice selected by respondents    \n",
    "data2['Choice'] = 0\n",
    "\n",
    "data2.loc[data2.query('(Q == \"a1\" & Q30 == \"Opción 1\") | (Q == \"a2\" & Q30 == \"Opción 2\") | (Q == \"a3\" & Q30 == \"Ninguno\") | \\\n",
    "    (Q == \"b1\" & Q32 == \"Opción 1\")| (Q == \"b2\" & Q32 == \"Opción 2\") | (Q == \"b3\" & Q32 == \"Ninguno\") | (Q == \"c1\" & Q34 == \"Opción 1\")| \\\n",
    "        (Q == \"c2\" & Q34 == \"Opción 2\") | (Q == \"c3\" & Q34 == \"Ninguno\") | (Q == \"d1\" & Q36 == \"Opción 1\")| (Q == \"d2\" & Q36 == \"Opción 2\") | \\\n",
    "            (Q == \"d3\" & Q36 == \"Ninguno\") | (Q == \"e1\" & Q38 == \"Opción 1\")| (Q == \"e2\" & Q38 == \"Opción 2\") | (Q == \"e3\" & Q38 == \"Ninguno\") | \\\n",
    "                (Q == \"f1\" & Q40 == \"Opción 1\")| (Q == \"f2\" & Q40 == \"Opción 2\") | (Q == \"f3\" & Q40 == \"Ninguno\") | (Q == \"g1\" & Q42 == \"Opción 1\")| \\\n",
    "                    (Q == \"g2\" & Q42 == \"Opción 2\") | (Q == \"g3\" & Q42 == \"Ninguno\") | (Q == \"h1\" & Q44 == \"Opción 1\")| (Q == \"h2\" & Q44 == \"Opción 2\") | \\\n",
    "                        (Q == \"h3\" & Q44 == \"Ninguno\") | (Q == \"i1\" & Q46 == \"Opción 1\")| (Q == \"i2\" & Q46 == \"Opción 2\") | (Q == \"i3\" & Q46 == \"Ninguno\") | \\\n",
    "                            (Q == \"j1\" & Q48 == \"Opción 1\")| (Q == \"j2\" & Q48 == \"Opción 2\") | (Q == \"j3\" & Q48 == \"Ninguno\") | (Q == \"k1\" & Q50 == \"Opción 1\")| \\\n",
    "                                (Q == \"k2\" & Q50 == \"Opción 2\") | (Q == \"k3\" & Q50 == \"Ninguno\") | (Q == \"l1\" & Q52 == \"Opción 1\")| (Q == \"l2\" & Q52 == \"Opción 2\") | \\\n",
    "                                    (Q == \"l3\" & Q52 == \"Ninguno\")').index, 'Choice'] = 1\n",
    "\n",
    "data2.iloc[:40,-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the columns you want to move to the front\n",
    "columns_to_move = ['Block', 'Set', 'Alt']\n",
    "df_to_move = data2[columns_to_move]\n",
    "\n",
    "# concatenate the selected columns with the rest of the dataframe\n",
    "data2 = pd.concat([df_to_move, data2.drop(columns_to_move, axis=1)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset as a .csv file named \"main_dataset_py.csv\" in the \\data_output directory\n",
    "\n",
    "#data2.to_csv(\"D:\\OneDrive - Texas Tech University\\PUBLICATIONS\\In Progress\\complex_supply_chain\\data_output\\main_dataset_py.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Second part of the script is about the setup of the experimental design in the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data of the experimental design into a pandas dataframe\n",
    "\n",
    "design = pd.read_stata(\"D:\\OneDrive - Texas Tech University\\PUBLICATIONS\\In Progress\\complex_supply_chain\\data_output\\exp_design.dta\")\n",
    "\n",
    "#design.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for each stage of the supply chain\n",
    "\n",
    "# create dummies for \"born\" column\n",
    "dummy_born = pd.get_dummies(design['born'], prefix='Born')\n",
    "dummy_born.rename(columns={'Born_mx': 'Born_mx', \n",
    "                           'Born_us': 'Born_us', \n",
    "                           'Born_tx': 'Born_tx', \n",
    "                           'Born_can': 'Born_can', \n",
    "                           'Born_nic': 'Born_nic'}, inplace=True)\n",
    "\n",
    "# create dummies for \"raised\" column\n",
    "raised_dummies = pd.get_dummies(design[\"raised\"], prefix=\"Raised\")\n",
    "raised_dummies.rename(columns={\"Raised_mx\": \"Raised_mx\",\n",
    "                               \"Raised_us\": \"Raised_us\",\n",
    "                               \"Raised_tx\": \"Raised_tx\",\n",
    "                               \"Raised_can\": \"Raised_can\",\n",
    "                               \"Raised_nic\": \"Raised_nic\"}, inplace=True)\n",
    "\n",
    "# create dummies for \"slaughtered\" column\n",
    "slaughtered_dummies = pd.get_dummies(design[\"slaughtered\"], prefix=\"Slaughtered\")\n",
    "slaughtered_dummies.rename(columns={\"Slaughtered_mx\": \"Slaughtered_mx\",\n",
    "                                     \"Slaughtered_us\": \"Slaughtered_us\",\n",
    "                                     \"Slaughtered_tx\": \"Slaughtered_tx\",\n",
    "                                     \"Slaughtered_can\": \"Slaughtered_can\",\n",
    "                                     \"Slaughtered_nic\": \"Slaughtered_nic\"}, inplace=True)\n",
    "\n",
    "\n",
    "# create a dummy variable for \"fsafety\" where 0=Standard and 1=Enhanced\n",
    "foodsafety_dummies = pd.get_dummies(design[\"fsafety\"], prefix=\"Fsafety\")\n",
    "foodsafety_dummies .rename(columns={\"Fsafety\": \"Fsafety\"}, inplace=True)\n",
    "\n",
    "# create a dummy variable for \"prod\" where 0=Approved and 1=Natural production practices\n",
    "prod_dummies = pd.get_dummies(design[\"prod\"], prefix=\"Prod\")\n",
    "prod_dummies.rename(columns={\"Prod\": \"Prod\"}, inplace=True)\n",
    "\n",
    "# concatenate the three dummy dataframes with the original dataframe\n",
    "design = pd.concat([design, dummy_born, raised_dummies, slaughtered_dummies, foodsafety_dummies, prod_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rename the new variables if needed\n",
    "design = design.rename(columns={'block':'Block', 'set':'Set', 'alt':'Alt', 'price':'Price', \\\n",
    "    'Born_BMexico':'Born_mx', 'Born_BUS':'Born_us', 'Born_BUS-TX':'Born_tx', 'Born_BCanada':'Born_can', 'Born_BNicaragua':'Born_nic', \\\n",
    "        'Raised_RMexico':'Raised_mx', 'Raised_RUS':'Raised_us', 'Raised_RUS-TX':'Raised_tx', 'Raised_RCanada':'Raised_can', 'Raised_RNicaragua':'Raised_nic', \\\n",
    "            'Slaughtered_1.0':'Slaughtered_mx', 'Slaughtered_2.0':'Slaughtered_us', 'Slaughtered_3.0':'Slaughtered_tx', 'Slaughtered_4.0':'Slaughtered_can', 'Slaughtered_5.0':'Slaughtered_nic', \\\n",
    "                })\n",
    "\n",
    "# insert a new column named \"ASC\" filled with zeros\n",
    "design['ASC'] = 0\n",
    "\n",
    "# drop the unnecesary columns in the dataframe\n",
    "design = design.drop('Prod_Approved', axis=1).drop('Fsafety_Standard', axis=1).drop('born', axis=1).drop('raised', axis=1).drop('slaughtered', axis=1).drop('fsafety', axis=1).drop('prod', axis=1)\n",
    "\n",
    "# remove the \"$\" sign from the values in the 'price' column\n",
    "design['Price'] = design['Price'].str.replace('$','', regex=False)\n",
    "\n",
    "# convert the 'price' column to numeric data type\n",
    "design['Price'] = pd.to_numeric(design['Price'])\n",
    "\n",
    "#design.iloc[:10,-20:]\n",
    "\n",
    "# convert the dataframe values to integers\n",
    "design = design.astype('int64')\n",
    "#print(design.dtypes)\n",
    "\n",
    "design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe\n",
    "columns = design.columns\n",
    "df_new = pd.DataFrame(np.zeros((36, len(columns))), columns=columns).astype('int64')\n",
    "\n",
    "df_new['Block'] = (df_new.index) // 12 + 1\n",
    "df_new['Set'] = 1 + ((df_new.index) % 12)\n",
    "df_new['Alt'] = 3\n",
    "df_new['ASC'] = 1\n",
    "\n",
    "print(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design2 = pd.concat([design, df_new])\n",
    "design2\n",
    "#design2.iloc[:,-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort data by Block and Alt\n",
    "design2 = design2.sort_values(by=['Block', 'Set', 'Alt', 'ASC'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the columns you want to move to the front\n",
    "columns_to_move2 = ['Block', 'Set', 'Alt']\n",
    "df_to_move2 = design2[columns_to_move2]\n",
    "\n",
    "# concatenate the selected columns with the rest of the dataframe\n",
    "design2 = pd.concat([df_to_move2, design2.drop(columns_to_move2, axis=1)], axis=1)\n",
    "\n",
    "design2.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset as a .csv file named \"dataset_design_py.csv\" in the \\data_output directory\n",
    "\n",
    "#design2.to_csv(\"D:\\OneDrive - Texas Tech University\\PUBLICATIONS\\In Progress\\complex_supply_chain\\data_output\\dataset_design_py.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Third part of the script is about the setup of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main dataset\n",
    "main_dataset = pd.read_csv(\"D:\\OneDrive - Texas Tech University\\PUBLICATIONS\\In Progress\\complex_supply_chain\\data_output\\main_dataset_py.csv\", low_memory=False)\n",
    "\n",
    "# Load experimental design dataset\n",
    "main_design = pd.read_csv(\"D:\\OneDrive - Texas Tech University\\PUBLICATIONS\\In Progress\\complex_supply_chain\\data_output\\dataset_design_py.csv\", low_memory=False)\n",
    "\n",
    "# Check if the three columns 'Block', 'Set', and 'Alt' exist in both dataframes\n",
    "if all(col in main_dataset.columns for col in ['Block', 'Set', 'Alt']) and all(col in main_design.columns for col in ['Block', 'Set', 'Alt']):\n",
    "    # Combine the three columns into one in both dataframes\n",
    "    main_dataset['Block Set Alt'] = main_dataset['Block'].astype(str) + ' ' + main_dataset['Set'].astype(str) + ' ' + main_dataset['Alt'].astype(str)\n",
    "    main_design['Block Set Alt'] = main_design['Block'].astype(str) + ' ' + main_design['Set'].astype(str) + ' ' + main_design['Alt'].astype(str)\n",
    "    \n",
    "    # Merge the two datasets using a one-to-one (1:1) merge on \"Block Set Alt\"\n",
    "    merged_dataset = main_dataset.merge(main_design, on='Block Set Alt', how='left')\n",
    "else:\n",
    "    print(\"Error: Columns 'Block', 'Set', or 'Alt' not found in one of the dataframes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset.iloc[:, :20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort data by Block, ID, Set, and Alt\n",
    "merged_dataset = merged_dataset.sort_values(by=['Block_x', 'ID', 'Set_x','Alt_x'])\n",
    "\n",
    "# Get the size of the dataframe\n",
    "rows, columns = merged_dataset.shape\n",
    "print(\"The dataframe has {} rows and {} columns\".format(rows, columns))\n",
    "\n",
    "#merged_dataset.iloc[:, -25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new variable named \"CHOICESITUATION\" that calculate the floor division of the row index divided by 3,\n",
    "merged_dataset['CHOICESITUATION'] = np.floor((merged_dataset.index)/3).astype(int) + 1\n",
    "\n",
    "#merged_dataset.iloc[:, -12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the new variables if needed\n",
    "merged_dataset = merged_dataset.rename(columns={'Block_x':'Block', 'Set_x':'Set', 'Alt_x':'Alt'})\n",
    "\n",
    "# drop the unnecesary columns in the dataframe\n",
    "merged_dataset = merged_dataset.drop('Block_y', axis=1).drop('Set_y', axis=1).drop('Alt_y', axis=1).drop('Block Set Alt', axis=1)\n",
    "\n",
    "# select the columns you want to move to the front\n",
    "columns_to_move3 = ['CHOICESITUATION', 'Block', 'Set', 'Alt', 'Choice', 'ASC', 'ID', 'Born_mx', 'Born_us', 'Born_tx', 'Born_can', 'Born_nic', 'Raised_mx', \\\n",
    "    'Raised_us', 'Raised_tx', 'Raised_can', 'Raised_nic', 'Slaughtered_mx', 'Slaughtered_us', 'Slaughtered_tx', 'Slaughtered_can', 'Slaughtered_nic', \\\n",
    "        'Price', 'Fsafety_Enhanced','Prod_Natural']\n",
    "\n",
    "df_to_move3 = merged_dataset[columns_to_move3]\n",
    "\n",
    "# concatenate the selected columns with the rest of the dataframe\n",
    "merged_dataset = pd.concat([df_to_move3, merged_dataset.drop(columns_to_move3, axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the size of the dataframe\n",
    "rows, columns = merged_dataset.shape\n",
    "print(\"The dataframe has {} rows and {} columns\".format(rows, columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset.iloc[:, :-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset as a .csv file named \"dataset_py.csv\" in the \\data_output directory\n",
    "\n",
    "#merged_dataset.to_csv(\"D:\\OneDrive - Texas Tech University\\PUBLICATIONS\\In Progress\\complex_supply_chain\\data_output\\dataset_py.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Forth and last part of the script and it is about the Willingness to pay analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Iterable abstract class was removed from collections in Python 3.10. We can use Iterable from collections.abc instead, or use Python 3.9\n",
    "\n",
    "!pip install pylogit\n",
    "!pip install biogeme\n",
    "#from collections.abc import Iterable\n",
    "\n",
    "import pandas as pd                    # For file input/output\n",
    "import numpy as np                     # For vectorized math operations\n",
    "#import pylogit as pl                   # For MNL model estimation and conversion from wide to long format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"D:\\OneDrive - Texas Tech University\\PUBLICATIONS\\In Progress\\complex_supply_chain\\data_output\\dataset_py.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 3 rows of the data transposed\n",
    "#dataset.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Steps\n",
    "\n",
    "1. Install biogeme by running !pip install biogeme in your Python environment.\n",
    "\n",
    "2. Prepare your dataset with the following columns:\n",
    "    An alternative ID column\n",
    "    A choice column (1 if the alternative was chosen, 0 otherwise)\n",
    "    Columns for each attribute of the alternatives\n",
    "    A column for the WTP associated with each attribute\n",
    "\n",
    "3. Define the model specification using the biogeme language, which is similar to the Stata syntax. Here is an example:\n",
    "\n",
    "'''\n",
    "\n",
    "ASC1 = Beta('ASC1', 0, None, None, 0)\n",
    "ASC2 = Beta('ASC2', 0, None, None, 1)\n",
    "B_PRICE = Beta('B_PRICE', 0, None, None, 0)\n",
    "B_QUALITY = Beta('B_QUALITY', 0, None, None, 0)\n",
    "B_WTP_PRICE = Beta('B_WTP_PRICE', 0, None, None, 0)\n",
    "B_WTP_QUALITY = Beta('B_WTP_QUALITY', 0, None, None, 0)\n",
    "\n",
    "MU = Beta('MU', 1, 0, None, 0)\n",
    "\n",
    "V1 = ASC1 + B_PRICE * PRICE + B_QUALITY * QUALITY + B_WTP_PRICE * WTP_PRICE1 + B_WTP_QUALITY * WTP_QUALITY1\n",
    "V2 = ASC2 + B_PRICE * PRICE + B_QUALITY * QUALITY + B_WTP_PRICE * WTP_PRICE2 + B_WTP_QUALITY * WTP_QUALITY2\n",
    "\n",
    "V = {1: V1, 2: V2}\n",
    "av = {1: 1, 2: 1}\n",
    "\n",
    "prob = bioLogit(V, av, CHOICE)\n",
    "logprob = log(MonteCarlo(prob))\n",
    "\n",
    "loglike = sum(logprob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' cont.\n",
    "\n",
    "In this example, PRICE and QUALITY are the attribute columns, and WTP_PRICE1, WTP_QUALITY1, WTP_PRICE2, and WTP_QUALITY2 are the WTP columns. \n",
    "ASC1 and ASC2 are alternative-specific constants, B_PRICE, B_QUALITY, B_WTP_PRICE, and B_WTP_QUALITY are the model parameters, and MU is the scale parameter. \n",
    "CHOICE is the column with the binary choice variable.\n",
    "\n",
    "4. Run the model estimation using the following code:\n",
    "\n",
    "'''\n",
    "model = bioModel(loglike)\n",
    "results = model.estimate()\n",
    "\n",
    "'''\n",
    "\n",
    "5. View the results using results.getEstimatedParameters(). \n",
    "This will give you the estimated values of the model parameters and their standard errors, t-statistics, and p-values. \n",
    "You can also use results.getGeneralStatistics() to view other model statistics, such as the log-likelihood and the number of observations.\n",
    "\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e47b1a34c05c1e3b83a62d7885c9d1b5ef8a0522d3be0182d0a008ec409b2b3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
